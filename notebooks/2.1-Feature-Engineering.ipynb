{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.config import config\n",
    "import warnings\n",
    "#np.random.seed(34)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "a031f8c9308754ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Feature Engineering\n",
    "Original features: 3 settings + 21 sensors = 24 features"
   ],
   "id": "5164ebbee8c53cd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index_names = ['unit_number', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i+1) for i in range(0,21)]"
   ],
   "id": "666713d9ad807e24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prepared_folder = config.PREPARED_DATA_PATH\n",
    "\n",
    "train_df = pd.read_csv(prepared_folder / \"train-all-prepared.csv\", index_col=False)\n",
    "test_df = pd.read_csv(prepared_folder / \"test-all-prepared.csv\", index_col=False)"
   ],
   "id": "d118d7df72599dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df",
   "id": "2a57ff0c44b82831",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Rolling Window Features\n",
    "Moving averages of sensor readings over last N cycles\n",
    "sensor_1_rolling_mean_5 = average of sensor 1 over last 5 cycles\n",
    "Smooths out noise, reveals underlying trends"
   ],
   "id": "d08b69cb17809d64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_rolling_features(df, window_sizes=[3, 5, 10]):\n",
    "    df_result = df.copy()\n",
    "\n",
    "    df_result = df_result.sort_values(['subset', 'unit_number', 'time_cycles'])\n",
    "\n",
    "    # Define feature columns (sensor readings)\n",
    "    sensor_cols = [col for col in df_result.columns if col.startswith('s_')]\n",
    "    settings_cols = [col for col in df_result.columns if col.startswith('setting_')]\n",
    "\n",
    "    print(f\"Creating rolling features for {len(sensor_cols)} sensor and {len(settings_cols)} settings columns...\")\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"  Processing window size {window_size}...\")\n",
    "\n",
    "        for col in sensor_cols + settings_cols:\n",
    "            # Use transform to maintain original index alignment\n",
    "            feature_name = f\"{col}_roll_{window_size}\"\n",
    "            df_result[feature_name] = (\n",
    "                df_result.groupby('unit_number')[col]\n",
    "                .rolling(window=window_size, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)  # Remove the groupby level from MultiIndex\n",
    "            )\n",
    "\n",
    "    return df_result"
   ],
   "id": "bf8cc1429e429d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating rolling features...\")\n",
    "window_sizes = [3, 5, 10, 20]\n",
    "#window_sizes = [3, 5, 10]\n",
    "#window_sizes = [2, 3, 5]\n",
    "#window_sizes = [2]\n",
    "train_df = create_rolling_features(train_df, window_sizes=window_sizes)\n",
    "test_df = create_rolling_features(test_df, window_sizes=window_sizes)"
   ],
   "id": "6f9a06210ce5f746",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rolling_cols = [c for c in train_df.columns if '_roll_' in c]\n",
    "correlations = train_df[rolling_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 rolling features correlated with RUL:\")\n",
    "print(correlations.head(11)[1:])  # Skip RUL itself\n",
    "\n",
    "# Check settings rolling features specifically\n",
    "setting_rolling = [c for c in rolling_cols if c.startswith('setting')]\n",
    "print(f\"\\nSettings rolling features correlation with RUL:\")\n",
    "for col in setting_rolling:\n",
    "    corr = train_df[col].corr(train_df['RUL'], method='spearman')\n",
    "    print(f\"{col}: {corr:.4f}\")"
   ],
   "id": "d8c3aa18fc578a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df",
   "id": "e1be03475782b1e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Delta/Rate Features\n",
    "How much each sensor changed from previous cycle\n",
    "Example: sensor_1_delta = current_value - previous_value\n",
    "Captures degradation speed/acceleration"
   ],
   "id": "84243965b0914fa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_delta_features(df):\n",
    "    df_sorted = df.sort_values(['subset', 'unit_number', 'time_cycles']).copy()\n",
    "\n",
    "    feature_cols = setting_names + sensor_names\n",
    "    print(f\"Creating delta features for all {len(feature_cols)} columns\")\n",
    "\n",
    "    # Calculate deltas within each unit\n",
    "    grouped = df_sorted.groupby('unit_number')\n",
    "    for col in feature_cols:\n",
    "        delta_name = f\"{col}_delta\"\n",
    "        df_sorted[delta_name] = grouped[col].diff().fillna(0)\n",
    "\n",
    "    delta_cols = len([c for c in df_sorted.columns if '_delta' in c])\n",
    "    print(f\"Created {delta_cols} delta features\")\n",
    "    return df_sorted"
   ],
   "id": "b4b00d940ddd588f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating delta features...\")\n",
    "train_df = create_delta_features(train_df)\n",
    "test_df = create_delta_features(test_df)"
   ],
   "id": "e368142aa0fd1e96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let correlation with RUL tell us which deltas are useful\n",
    "delta_cols = [c for c in train_df.columns if '_delta' in c]\n",
    "delta_correlations = train_df[delta_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 delta features correlated with RUL:\")\n",
    "print(delta_correlations.head(10))"
   ],
   "id": "5028ff6f6d3468e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Time-based Features\n",
    "What: Normalized cycle position (0 to 1 across engine's life)\n",
    "Example: cycle_norm = current_cycle / max_cycles_for_this_engine"
   ],
   "id": "abd2a202d5639db6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "❗❗❗ Won't do - skip this FE ❗❗❗\n",
    "Train has data until faillure\n",
    "Test has data until a certain point, and need to predict RUL from this point (cycle)\n",
    "They cannot be normalized the same way, this will create issue in prediction"
   ],
   "id": "afcd7c5bdc197fcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Interaction Features (Medium complexity)\n",
    "What: Combine operating settings to capture complex conditions\n",
    "Example: setting_1_x_setting_2 = setting_1 * setting_2\n",
    "Why important: Equipment might behave differently under combined stress\n",
    "Computation: Simple multiplication"
   ],
   "id": "4fc988f592cf61ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Operating settings with each other (setting_1 × setting_2 × setting_3)\n",
    "Settings with key sensors (high temperature + high pressure scenarios)\n",
    "Physically related sensors (temperature sensors with pressure sensors)"
   ],
   "id": "3b1d71ddec085977"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`X` : captures \"amplification\" effects\n",
    "`+` : captures \"combined stress\" effects\n",
    "`Ratio/Division`: captures \"efficiency\" or normalized response"
   ],
   "id": "a965ac5c0ae9b275"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Settings × Settings: 3 features (systematic)",
   "id": "45e55c50fcbca0e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_settings_x_settings_interaction_features(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['setting_1_x_setting_2'] = df_copy['setting_1'] * df_copy['setting_2']\n",
    "    df_copy['setting_1_x_setting_3'] = df_copy['setting_1'] * df_copy['setting_3']\n",
    "    df_copy['setting_2_x_setting_3'] = df_copy['setting_2'] * df_copy['setting_3']\n",
    "    interaction_cols = [c for c in df_copy.columns if c not in df.columns]\n",
    "    print(f\"Created {len(interaction_cols)} interaction for settings features\")\n",
    "\n",
    "    return df_copy, interaction_cols"
   ],
   "id": "58903e8394d5ce38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating settings x settings features...\")\n",
    "train_df, settings_interaction_cols = create_settings_x_settings_interaction_features(train_df)\n",
    "test_df, _ = create_settings_x_settings_interaction_features(test_df)"
   ],
   "id": "26824ec59b628eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "settings_interaction_corr = train_df[settings_interaction_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Settings×settings interactions correlated with RUL:\")\n",
    "print(settings_interaction_corr.head(4))"
   ],
   "id": "8f3e94c638bc29aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Settings × Sensors: 63 features (systematic)",
   "id": "248904cab552e60c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_settings_sensor_interactions(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    interaction_count = 0\n",
    "\n",
    "    # Each setting × each sensor\n",
    "    for setting in setting_names:\n",
    "        for sensor in sensor_names:\n",
    "            feature_name = f\"{setting}_x_{sensor}\"\n",
    "            df_copy[feature_name] = df_copy[setting] * df_copy[sensor]\n",
    "            interaction_count += 1\n",
    "\n",
    "    print(f\"Created {interaction_count} settings×sensors interaction features\")\n",
    "    print(f\"({len(setting_names)} settings × {len(sensor_names)} sensors)\")\n",
    "\n",
    "    return df_copy\n"
   ],
   "id": "e18e7b719d826ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating settings×sensors interaction features...\")\n",
    "train_df = create_settings_sensor_interactions(train_df)\n",
    "test_df = create_settings_sensor_interactions(test_df)"
   ],
   "id": "e02221665f16261a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "settings_sensor_cols = [c for c in train_df.columns if c.startswith('setting_') and '_x_s_' in c]\n",
    "settings_sensor_corr = train_df[settings_sensor_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 settings×sensors interactions correlated with RUL:\")\n",
    "print(settings_sensor_corr.head(10))"
   ],
   "id": "55fd125e071faf0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3 Sensors x sensors:\n",
    "Instead of all 210 possible combinations, let's group them by what they measure."
   ],
   "id": "2ea7f0911b33bf58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 4.3.1 Temperature Group",
   "id": "a515037cf4a2af88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_temperature_group_interaction_features(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_copy['temp_fan_to_lpc'] = df_copy['s_1'] * df_copy['s_2']           # Fan inlet × LPC outlet\n",
    "    df_copy['temp_compression_ratio'] = df_copy['s_3'] / df_copy['s_2']    # HPC outlet / LPC outlet\n",
    "    df_copy['temp_expansion_ratio'] = df_copy['s_4'] / df_copy['s_3']      # LPT outlet / HPC outlet\n",
    "    df_copy['temp_overall_rise'] = df_copy['s_3'] - df_copy['s_1']         # HPC outlet - Fan inlet\n",
    "\n",
    "    interaction_cols = [c for c in df_copy.columns if c not in df.columns]\n",
    "    print(f\"Created {len(interaction_cols)} interaction for temperature features\")\n",
    "\n",
    "    return df_copy, interaction_cols"
   ],
   "id": "d0da63b4decc8e54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating temperature features...\")\n",
    "train_df, interaction_cols = create_temperature_group_interaction_features(train_df)\n",
    "test_df, _ = create_temperature_group_interaction_features(test_df)"
   ],
   "id": "734dc52fc3265ccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "settings_interaction_corr = train_df[interaction_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Settings×settings interactions correlated with RUL:\")\n",
    "print(settings_interaction_corr.head(10))"
   ],
   "id": "368f499617e51b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 4.3.2 Pressure Group",
   "id": "ce0fcb09c5a6165e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_pressure_group_interaction_features(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_copy['pressure_ratio_fan'] = df_copy['s_7'] / (df_copy['s_5'] + 1e-6)         # HPC outlet / Fan inlet\n",
    "    df_copy['pressure_bypass_vs_core'] = df_copy['s_6'] / (df_copy['s_7'] + 1e-6)    # Bypass / HPC outlet\n",
    "    df_copy['pressure_drop_turbine'] = df_copy['s_7'] - df_copy['s_11']              # HPC outlet - HPC static\n",
    "\n",
    "    interaction_cols = [c for c in df_copy.columns if c not in df.columns]\n",
    "    print(f\"Created {len(interaction_cols)} interaction for pressure features\")\n",
    "\n",
    "    return df_copy, interaction_cols"
   ],
   "id": "11cc81717693a0fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating pressure group interaction features...\")\n",
    "train_df, pressure_interaction_cols = create_pressure_group_interaction_features(train_df)\n",
    "test_df, _ = create_pressure_group_interaction_features(test_df)"
   ],
   "id": "dc75409a878124aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "pressure_interaction_corr = train_df[pressure_interaction_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Pressure group interactions correlated with RUL:\")\n",
    "print(pressure_interaction_corr.head(10))"
   ],
   "id": "1ff4dc546e50474e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 4.3.3 Speed Group",
   "id": "732a7b3d9a877ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Speed Group Interaction Features\n",
    "def create_speed_group_interaction_features(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_copy['speed_fan_core_ratio'] = df_copy['s_8'] / (df_copy['s_9'] + 1e-6)       # Physical fan / Physical core\n",
    "    df_copy['speed_corrected_ratio'] = df_copy['s_13'] / (df_copy['s_14'] + 1e-6)    # Corrected fan / Corrected core\n",
    "    df_copy['speed_efficiency'] = df_copy['s_8'] / (df_copy['s_18'] + 1e-6)          # Physical / Required fan speed\n",
    "\n",
    "    interaction_cols = [c for c in df_copy.columns if c not in df.columns]\n",
    "    print(f\"Created {len(interaction_cols)} interaction for speed features\")\n",
    "\n",
    "    return df_copy, interaction_cols"
   ],
   "id": "d811fcdaaa8466e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating speed group interaction features...\")\n",
    "train_df, speed_interaction_cols = create_speed_group_interaction_features(train_df)\n",
    "test_df, _ = create_speed_group_interaction_features(test_df)"
   ],
   "id": "a0cb7e8e47c79cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "speed_interaction_corr = train_df[speed_interaction_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Speed group interactions correlated with RUL:\")\n",
    "print(speed_interaction_corr.head(10))"
   ],
   "id": "654820f2ec46229d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 4.3.4 Fuel & Air Group",
   "id": "57e0a203da2aa36f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fuel & Air Group Interaction Features\n",
    "def create_fuel_air_group_interaction_features(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_copy['fuel_air_efficiency'] = df_copy['s_12'] * df_copy['s_16']                          # Fuel flow ratio × Burner fuel-air\n",
    "    df_copy['cooling_air_total'] = df_copy['s_20'] + df_copy['s_21']                            # HP + LP turbine cooling air\n",
    "    df_copy['fuel_to_cooling'] = df_copy['s_12'] / (df_copy['s_20'] + df_copy['s_21'] + 1e-6)   # Fuel efficiency vs cooling\n",
    "\n",
    "    interaction_cols = [c for c in df_copy.columns if c not in df.columns]\n",
    "    print(f\"Created {len(interaction_cols)} interaction for fuel & air features\")\n",
    "\n",
    "    return df_copy, interaction_cols"
   ],
   "id": "b29ea06865e241de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating fuel & air group interaction features...\")\n",
    "train_df, fuel_air_interaction_cols = create_fuel_air_group_interaction_features(train_df)\n",
    "test_df, _ = create_fuel_air_group_interaction_features(test_df)"
   ],
   "id": "2ce67f6fe6c06eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "fuel_air_interaction_corr = train_df[fuel_air_interaction_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Fuel & air group interactions correlated with RUL:\")\n",
    "print(fuel_air_interaction_corr.head(10))"
   ],
   "id": "308bb1e4af506693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 4.3.5 Cross-System Group (2 interactions)",
   "id": "66d417b25af0e42e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cross-System Group Interaction Features\n",
    "def create_cross_system_group_interaction_features(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_copy['thermal_pressure_stress'] = df_copy['s_3'] * df_copy['s_7']    # HPC temp × HPC pressure (most stressed point)\n",
    "    df_copy['engine_load_indicator'] = df_copy['s_10'] * df_copy['s_15']    # Engine pressure ratio × Bypass ratio\n",
    "\n",
    "    interaction_cols = [c for c in df_copy.columns if c not in df.columns]\n",
    "    print(f\"Created {len(interaction_cols)} interaction for cross-system features\")\n",
    "\n",
    "    return df_copy, interaction_cols"
   ],
   "id": "91739d835dc1bae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Creating cross-system group interaction features...\")\n",
    "train_df, cross_system_interaction_cols = create_cross_system_group_interaction_features(train_df)\n",
    "test_df, _ = create_cross_system_group_interaction_features(test_df)"
   ],
   "id": "6c0ff3706e709ec6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check correlations with RUL\n",
    "cross_system_interaction_corr = train_df[cross_system_interaction_cols + ['RUL']].corr(method='spearman')['RUL'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Cross-system group interactions correlated with RUL:\")\n",
    "print(cross_system_interaction_corr.head(10))"
   ],
   "id": "74c0c452e3a06780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_____",
   "id": "fc73e59b74b65060"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Selection",
   "id": "785466fb2d6abfdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.shape",
   "id": "2608710db2d08156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_df.shape",
   "id": "d55afb7b630496de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Planning to use Random Forest, should be able to handle ~200 features",
   "id": "5fc9ecec1b7e1524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_df.columns",
   "id": "ef1330facda774b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Random Forest Feature Importance",
   "id": "aa3ee2a4f294cd36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare features and target\n",
    "#train_to_use = train_df[train_df['subset'] == 1]\n",
    "#test_to_use = test_df[test_df['subset'] == 1]\n",
    "\n",
    "train_to_use = train_df\n",
    "for subset_id in [1, 2, 3, 4]:\n",
    "    mask = train_to_use['subset'] == subset_id\n",
    "    train_to_use.loc[mask, 'unit_number'] += 1000 * subset_id\n",
    "\n",
    "test_to_use = test_df\n",
    "for subset_id in [1, 2, 3, 4]:\n",
    "    mask = test_to_use['subset'] == subset_id\n",
    "    test_to_use.loc[mask, 'unit_number'] += 1000 * subset_id"
   ],
   "id": "bfc058e060532631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rul_thresholds = {\n",
    "    1: {'max': 145, 'min': 6},\n",
    "    2: {'max': 194, 'min': 6},\n",
    "    3: {'max': 145, 'min': 6},\n",
    "    4: {'max': 194, 'min': 6}\n",
    "}\n",
    "\n",
    "# Apply different RUL filtering for each subset\n",
    "filtered_dfs = []\n",
    "for subset_id in [1, 2, 3, 4]:\n",
    "    subset_data = train_to_use[train_to_use['subset'] == subset_id]\n",
    "    max_rul = rul_thresholds[subset_id]['max']\n",
    "    min_rul = rul_thresholds[subset_id]['min']\n",
    "\n",
    "    filtered_subset = subset_data[\n",
    "        (subset_data['RUL'] <= max_rul) &\n",
    "        (subset_data['RUL'] >= min_rul)\n",
    "    ]\n",
    "    filtered_dfs.append(filtered_subset)\n",
    "\n",
    "# Combine all filtered subsets back together\n",
    "train_to_use = pd.concat(filtered_dfs, ignore_index=True)"
   ],
   "id": "16189fb69b64c260",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get unique unit numbers for splitting\n",
    "unique_units = train_to_use['unit_number'].unique()\n",
    "train_units, valid_units = train_test_split(unique_units, test_size=0.1, random_state=45)\n",
    "\n",
    "# Split data based on unit numbers\n",
    "train_mask = train_to_use['unit_number'].isin(train_units)\n",
    "test_mask = train_to_use['unit_number'].isin(valid_units)\n",
    "\n",
    "X_train = train_to_use[train_mask].drop(['unit_number', 'time_cycles', 'RUL'], axis=1)\n",
    "y_train = train_to_use[train_mask]['RUL']\n",
    "\n",
    "X_valid = train_to_use[test_mask].drop(['unit_number', 'time_cycles', 'RUL'], axis=1)\n",
    "y_valid = train_to_use[test_mask]['RUL']"
   ],
   "id": "4802db47633b42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Scale features (keep subset as is since it's categorical)\n",
    "scaler = MinMaxScaler()\n",
    "feature_cols = [col for col in X_train.columns if col != 'subset']\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_valid_scaled = X_valid.copy()\n",
    "\n",
    "X_train_scaled[feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "X_valid_scaled[feature_cols] = scaler.transform(X_valid[feature_cols])\n"
   ],
   "id": "333cf89b2839add7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,           # More trees for complex patterns\n",
    "    max_depth=None,             # Let trees grow deep\n",
    "    min_samples_split=5,        # Prevent overfitting\n",
    "    min_samples_leaf=2,         # Balance bias-variance\n",
    "    max_features='sqrt',       # Features per tree: √200 ~ 14 features per tree\n",
    "    random_state=46,\n",
    "    n_jobs=-1                   # XGBoost for GPU\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "top_n_features = X_train_scaled.columns"
   ],
   "id": "a1cdf19b2ad88461",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20))"
   ],
   "id": "34e4e1fbe108661c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_50_features = feature_importance.head(50)['feature'].tolist()\n",
    "print(f\"\\nSelected {len(top_50_features)} features\")\n",
    "print(f\"Baseline performance - RMSE: {np.sqrt(((rf.predict(X_valid_scaled) - y_valid)**2).mean()):.2f}\")"
   ],
   "id": "18690e1e678f8115",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-> Iterate here to find best hyperparameter (and feature engineering - rolling windows sizes, etc) -> script, optuna, MLFlow\n",
    "max_train_RUL_to_use, etc"
   ],
   "id": "35cc0b87f7a5689e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Full train with feature selection",
   "id": "511faf5a46331664"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n = 300\n",
    "top_n_features = feature_importance['feature'].iloc[-n:]  # Last N feature names\n",
    "top_n_features"
   ],
   "id": "e29e5c73b0fd7e52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#X_train = train_to_use[top_n_features]\n",
    "X_train = train_to_use.drop(['unit_number', 'time_cycles', 'RUL'], axis=1)\n",
    "y_train = train_to_use['RUL']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "feature_cols = [col for col in X_train.columns if col != 'subset']\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=rf.n_estimators,               # More trees for complex patterns\n",
    "    max_depth=rf.max_depth,                     # Let trees grow deep\n",
    "    min_samples_split=rf.min_samples_split,     # Prevent overfitting\n",
    "    min_samples_leaf=rf.min_samples_leaf,       # Balance bias-variance\n",
    "    max_features=rf.max_features,               # Features per tree: √200 ~ 14 features per tree\n",
    "    random_state=rf.random_state,\n",
    "    n_jobs=-1                                   # XGBoost for GPU\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)"
   ],
   "id": "20b7b2f1616c7083",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predict on Test",
   "id": "5d32d0683a70ed94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_summary = test_to_use.groupby('unit_number').agg({\n",
    "    'time_cycles': 'max',\n",
    "    'RUL': 'min'\n",
    "}).reset_index()"
   ],
   "id": "6087baa6c0913070",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare test data - only last row per engine unit for RUL prediction\n",
    "# First, let's verify that the last row (max time_cycles) is what we want to predict on\n",
    "test_summary = test_to_use.groupby('unit_number').agg({\n",
    "    'time_cycles': 'max',\n",
    "    'RUL': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "# Assert that for each unit, there's only one row with max time_cycles\n",
    "for unit in test_to_use['unit_number'].unique():\n",
    "    unit_data = test_to_use[test_to_use['unit_number'] == unit]\n",
    "    max_time_rows = unit_data[unit_data['time_cycles'] == unit_data['time_cycles'].max()]\n",
    "    assert len(max_time_rows) == 1, f\"Unit {unit}: Multiple rows with same max time_cycles\"\n",
    "\n",
    "print(\"✓ Verified: Each engine unit has exactly one row with maximum time_cycles\")\n",
    "\n",
    "# Get only the last row (highest time_cycles) for each engine unit\n",
    "test_last_rows = test_to_use.loc[test_to_use.groupby('unit_number')['time_cycles'].idxmax()]\n",
    "\n",
    "print(f\"Original test data shape: {test_to_use.shape}\")\n",
    "print(f\"Test data (last rows only) shape: {test_last_rows.shape}\")\n",
    "\n",
    "#X_test = test_last_rows[top_n_features]\n",
    "X_test = test_last_rows.drop(['unit_number', 'time_cycles', 'RUL'], axis=1)\n",
    "y_test = test_last_rows['RUL']\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[feature_cols] = scaler.transform(X_test[feature_cols])\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Optional: Additional metrics\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test R²: {r2:.3f}\")\n",
    "\n",
    "# Optional: Prediction vs Actual plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual RUL')\n",
    "plt.ylabel('Predicted RUL')\n",
    "plt.title(f'Predictions vs Actual (RMSE: {rmse:.2f})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "66ede13eaf967347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# More trees: n_estimators=500\n",
    "# Tune hyperparameters: max_depth, min_samples_split\n",
    "# Keep top N features only: top_30_features = feature_importance.head(30)['feature'].tolist()"
   ],
   "id": "57e05325b7416227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Mutual Information",
   "id": "90584afef3e96d3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "#\n",
    "# mi_scores = mutual_info_regression(X, y)\n",
    "# mi_df = pd.DataFrame({\n",
    "#     'feature': X.columns,\n",
    "#     'mutual_info': mi_scores\n",
    "# }).sort_values('mutual_info', ascending=False)"
   ],
   "id": "f1389aa12d3bb38b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Recursive Feature Elimination (RFE)",
   "id": "b67e62e2a1035171"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "#\n",
    "# rfe = RFE(estimator=RandomForestRegressor(n_estimators=100),\n",
    "#           n_features_to_select=50)\n",
    "# rfe.fit(X, y)\n",
    "# selected_features = X.columns[rfe.support_]"
   ],
   "id": "7655fa33dfe6175f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Variance Threshold",
   "id": "ebb8b72f343a615d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "#\n",
    "# # Remove features with very low variance\n",
    "# selector = VarianceThreshold(threshold=0.01)\n",
    "# X_filtered = selector.fit_transform(X)"
   ],
   "id": "a84a4fc9e3af9b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_____",
   "id": "bcfffd813dd70804"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cycle selection - if late submit for competition",
   "id": "ca05b05e2f71961c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Idea for later: Since we pre-compute time feature, maybe we could only train on cycle that are close to RUL.\n",
    "Need to analyse RUL distribution of test.\n",
    "Need to make sure we only receive last cycle to predict for competition"
   ],
   "id": "2ae8d8673e62f17d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
