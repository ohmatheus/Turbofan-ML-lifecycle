{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Multiple Scenarios:",
   "id": "e4c21a0ab30f56d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scenario 1 (Competition): One model for all subsets (001, 002, 003, 004)\n",
    "\n",
    "→ Merge all 4, then split, then scale"
   ],
   "id": "f29b3d9d197cd0b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scenario 2 (Competition): Separate model per subset\n",
    "\n",
    "→ 4 separate splits + 4 separate scalers etc"
   ],
   "id": "fa6a159584f1bb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scenario 3 (Drift simulation, this project): One model, retained when drift detected\n",
    "\n",
    "-> Train first on 001 train (split, scale), then when drift detected, 001 + 002, then if drift.. etc\n",
    "\n",
    "This is the strategy we will go for. So we will only use this notebook as a sandbox, to write functions and automate data loading and preprocessing."
   ],
   "id": "430ccb47832b250e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "But I keep the 2 previous scenario in mind in case I want to do a 'late submission' for the competition.",
   "id": "6b0c8ffd1f5bf013"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_____",
   "id": "1c3d66df64a14b8a"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils.config import config\n",
    "import warnings\n",
    "np.random.seed(34)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "7baba2d5234739f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load from raw - prepare",
   "id": "67e004d26f36e408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "subsets = ['001', '002', '003', '004']",
   "id": "ea3876f96ee9c3e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index_names = ['unit_number', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i+1) for i in range(0,21)]\n",
    "col_names = index_names + setting_names + sensor_names"
   ],
   "id": "3a824e4dd3c439c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_data_path = config.RAW_DATA_PATH / 'CMaps/'\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for fd_num in subsets:\n",
    "    datasets[fd_num] = {\n",
    "        'train': pd.read_csv(raw_data_path / f'train_FD{fd_num}.txt', sep='\\s+', header=None, index_col=False, names=col_names),\n",
    "        'test': pd.read_csv(raw_data_path / f'test_FD{fd_num}.txt', sep='\\s+', header=None, index_col=False, names=col_names),\n",
    "        'rul': pd.read_csv(raw_data_path / f'RUL_FD{fd_num}.txt', sep='\\s+', header=None, index_col=False, names=['RUL'])\n",
    "    }"
   ],
   "id": "32f38dfec4c6c5bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fd_num in subsets:\n",
    "    train = datasets[fd_num]['train']\n",
    "    valid = datasets[fd_num]['test']\n",
    "    print(f'FD{fd_num}:')\n",
    "    print(f'  Train shape: {train.shape}')\n",
    "    print(f'  Test shape: {valid.shape}')\n",
    "    print(f'  Test %: {len(valid)/(len(valid)+len(train))*100.0:.3f}')\n",
    "    print()"
   ],
   "id": "485ffae5e06e5f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets['001']['rul']",
   "id": "cac4f9196bb811fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_RUL_column(df, ref_rul=None):\n",
    "    train_grouped_by_unit = df.groupby(by='unit_number')\n",
    "    max_time_cycles = train_grouped_by_unit['time_cycles'].max()\n",
    "    merged = df.merge(max_time_cycles.to_frame(name='max_time_cycle'), left_on='unit_number', right_index=True)\n",
    "\n",
    "    if ref_rul is not None:\n",
    "        # For test data with reference RUL\n",
    "        # Extract values from DataFrame (first column)\n",
    "        ref_rul_values = ref_rul.iloc[:, 0]\n",
    "\n",
    "        unique_units = df['unit_number'].nunique()\n",
    "        assert len(ref_rul_values) == unique_units, f\"RUL count ({len(ref_rul_values)}) ≠ unique units ({unique_units})\"\n",
    "\n",
    "        unit_numbers = sorted(df['unit_number'].unique())\n",
    "        rul_mapping = dict(zip(unit_numbers, ref_rul_values))\n",
    "        merged['ref_rul'] = merged['unit_number'].map(rul_mapping)\n",
    "        merged[\"RUL\"] = merged['ref_rul'] + (merged[\"max_time_cycle\"] - merged['time_cycles'])\n",
    "        merged = merged.drop([\"max_time_cycle\", \"ref_rul\"], axis=1)\n",
    "    else:\n",
    "        # For training data\n",
    "        merged[\"RUL\"] = merged[\"max_time_cycle\"] - merged['time_cycles']\n",
    "        merged = merged.drop(\"max_time_cycle\", axis=1)\n",
    "\n",
    "    return merged\n",
    "\n",
    "# Apply to all datasets\n",
    "for fd_num in subsets:\n",
    "    datasets[fd_num]['train'] = add_RUL_column(datasets[fd_num]['train'])\n",
    "    datasets[fd_num]['test'] = add_RUL_column(datasets[fd_num]['test'], datasets[fd_num]['rul'])"
   ],
   "id": "83c5212ce671bf0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets['001']['train'].columns",
   "id": "4f9e43356ddd5be0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets['001']['test'].columns",
   "id": "307135817f8ced82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge all, add columns for subset source, and save as 1 prepared dataframe",
   "id": "2dbb83f22d5dcbe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for fd_num in subsets:\n",
    "    train_subset = datasets[fd_num]['train'].copy()\n",
    "    train_subset['subset'] = fd_num\n",
    "    train_data.append(train_subset)\n",
    "\n",
    "    test_subset = datasets[fd_num]['test'].copy()\n",
    "    test_subset['subset'] = fd_num\n",
    "    test_data.append(test_subset)\n",
    "\n",
    "# Concatenate all train and test dataframes\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "processed_dir = config.PREPARED_DATA_PATH\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_df.to_csv(processed_dir / 'train-all-prepared.csv', index=False)\n",
    "test_df.to_csv(processed_dir / 'test-all-prepared.csv', index=False)\n",
    "\n",
    "print(f\"Train dataset shape: {train_df.shape}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "print(\"CSV files saved successfully!\")"
   ],
   "id": "c7eb5e219ab27bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_________________",
   "id": "77c1632f481038e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# #we do not want to scale the index and the settings\n",
    "# drop_labels = index_names+setting_names\n",
    "#\n",
    "# #for fd_num in subsets:\n",
    "# X_train=datasets['001']['train'].drop(columns=drop_labels).copy()\n",
    "# X_train, X_test, y_train, y_test=train_test_split(X_train,X_train['RUL'], test_size=0.3, random_state=42)\n",
    "# scaler = MinMaxScaler()\n",
    "# #Droping the target variable\n",
    "# X_train.drop(columns=['RUL'], inplace=True)\n",
    "# X_test.drop(columns=['RUL'], inplace=True)\n",
    "# #Scaling X_train and X_test\n",
    "# X_train_s=scaler.fit_transform(X_train)\n",
    "# X_test_s=scaler.transform(X_test)\n",
    "# #Conserve only the last occurence of each unit to match the length of y_valid\n",
    "# X_valid = datasets['001']['test'].groupby('unit_number').last().reset_index().drop(columns=drop_labels)\n",
    "# #scaling X_valid\n",
    "# X_valid_s=scaler.transform(X_valid)\n"
   ],
   "id": "32bfae0dcf83b977",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
