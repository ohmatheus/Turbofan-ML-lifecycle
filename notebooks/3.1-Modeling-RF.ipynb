{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.models.random_forest_utils import fit_rf, eval_rul, plot_rmse, Metrics\n",
    "\n",
    "from src.data_utils.feature_engineering import create_features, FeatureEngineeringSettings\n",
    "from src.utils.config import config\n",
    "\n",
    "# np.random.seed(34)\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8dbc723f13241dd",
   "metadata": {},
   "source": "## Load and apply FE"
  },
  {
   "cell_type": "code",
   "id": "20ddd288ddc63496",
   "metadata": {},
   "source": [
    "fe_settings = FeatureEngineeringSettings()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c14fa09e39f9b3eb",
   "metadata": {},
   "source": [
    "prepared_folder = config.PREPARED_DATA_PATH\n",
    "\n",
    "train_df = pd.read_csv(prepared_folder / \"train-all-prepared.csv\", index_col=False)\n",
    "test_df = pd.read_csv(prepared_folder / \"test-all-prepared.csv\", index_col=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "147be16321f4003b",
   "metadata": {},
   "source": [
    "train_df = create_features(train_df, fe_settings)\n",
    "test_df = create_features(test_df, fe_settings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97f5fb892b276b34",
   "metadata": {},
   "source": [
    "train_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d4e86b954958b48",
   "metadata": {},
   "source": [
    "train_to_use = train_df\n",
    "test_to_use = test_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d96b1250f4de1ed",
   "metadata": {},
   "source": "## Filtering"
  },
  {
   "cell_type": "markdown",
   "id": "7ccfbbac6d974238",
   "metadata": {},
   "source": "We can only select trainning data among a preselected range of RUL (corresponding to those in 'test' subset)"
  },
  {
   "cell_type": "code",
   "id": "ba4ed75df3cd3f4f",
   "metadata": {},
   "source": [
    "rul_thresholds = {\n",
    "    1: {\"max\": 145, \"min\": 6},\n",
    "    2: {\"max\": 194, \"min\": 6},\n",
    "    3: {\"max\": 145, \"min\": 6},\n",
    "    4: {\"max\": 194, \"min\": 6},\n",
    "}\n",
    "\n",
    "# Apply different RUL filtering for each subset\n",
    "filtered_dfs = []\n",
    "for subset_id in [1, 2, 3, 4]:\n",
    "    subset_data = train_df[train_df[\"subset\"] == subset_id]\n",
    "    max_rul = rul_thresholds[subset_id][\"max\"]\n",
    "    min_rul = rul_thresholds[subset_id][\"min\"]\n",
    "\n",
    "    filtered_subset = subset_data[(subset_data[\"RUL\"] <= max_rul) & (subset_data[\"RUL\"] >= min_rul)]\n",
    "    filtered_dfs.append(filtered_subset)\n",
    "\n",
    "# Combine all filtered subsets back together\n",
    "train_to_use = pd.concat(filtered_dfs, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48655b5392bb05ab",
   "metadata": {},
   "source": "# 1. Train CV"
  },
  {
   "cell_type": "markdown",
   "id": "b5c14a48fa18c488",
   "metadata": {},
   "source": "Iterate here to find best hyperparameter (and feature engineering - rolling windows sizes, etc) -> script, optuna, MLFlow, also parametrize RUL range to train as HP."
  },
  {
   "cell_type": "code",
   "id": "8fc12b96da4201d1",
   "metadata": {},
   "source": [
    "# best_model, val_rmse = fit_rf(train_to_use, param_grid)\n",
    "best_model, val_rmse = fit_rf(train_to_use)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3f6985dc5859e28",
   "metadata": {},
   "source": "y_pred, y_test, metrics = eval_rul(best_model, test_to_use)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa2cc3b36f843152",
   "metadata": {},
   "source": [
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe1c1615a45dbae1",
   "metadata": {},
   "source": "## 2.2 Mutual Information"
  },
  {
   "cell_type": "code",
   "id": "78fea03d11aafc8d",
   "metadata": {},
   "source": [
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "#\n",
    "# mi_scores = mutual_info_regression(X, y)\n",
    "# mi_df = pd.DataFrame({\n",
    "#     'feature': X.columns,\n",
    "#     'mutual_info': mi_scores\n",
    "# }).sort_values('mutual_info', ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccf1846522ebbad9",
   "metadata": {},
   "source": "## 2.3 Recursive Feature Elimination (RFE)"
  },
  {
   "cell_type": "code",
   "id": "591b5902e87c0597",
   "metadata": {},
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "#\n",
    "# rfe = RFE(estimator=RandomForestRegressor(n_estimators=100),\n",
    "#           n_features_to_select=50)\n",
    "# rfe.fit(X, y)\n",
    "# selected_features = X.columns[rfe.support_]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e52fd347e3ff6d4",
   "metadata": {},
   "source": "## 2.4 Variance Threshold"
  },
  {
   "cell_type": "code",
   "id": "20d30b442750a976",
   "metadata": {},
   "source": [
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "#\n",
    "# # Remove features with very low variance\n",
    "# selector = VarianceThreshold(threshold=0.01)\n",
    "# X_filtered = selector.fit_transform(X)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4aeb2dc1a53f02b1",
   "metadata": {},
   "source": "_____"
  },
  {
   "cell_type": "markdown",
   "id": "13602952b4d408a5",
   "metadata": {},
   "source": "# 3. Simulate adding categories one after the other (for model lifecycle simulation)"
  },
  {
   "cell_type": "markdown",
   "id": "ab88d9ca8c665e00",
   "metadata": {},
   "source": [
    "This is not ML research, but more to validate this will work in this simulation model lifecycle project\n",
    "Each time using the `train` subset for trainning - and `test` subset for testing\n",
    "So for instance: 001_train eval on 002_test"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d99b024900c1fa5",
   "metadata": {},
   "source": [
    "train_to_use"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e157d2fde40e2471",
   "metadata": {},
   "source": [
    "def subset_dfs(df: pd.DataFrame) -> list[pd.DataFrame]:\n",
    "    subset_dfs: list[pd.DataFrame] = []\n",
    "    for subset_id in [1, 2, 3, 4]:\n",
    "        mask = df[\"subset\"] == subset_id\n",
    "        subset_dfs.append(df[mask].copy())\n",
    "    return subset_dfs\n",
    "\n",
    "\n",
    "subset_train_dfs: list[pd.DataFrame] = subset_dfs(train_to_use)\n",
    "subset_test_dfs: list[pd.DataFrame] = subset_dfs(test_to_use)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9133943323dd7ecf",
   "metadata": {},
   "source": [
    "FD001_train = subset_train_dfs[0]\n",
    "FD002_train = subset_train_dfs[1]\n",
    "FD003_train = subset_train_dfs[2]\n",
    "FD004_train = subset_train_dfs[3]\n",
    "\n",
    "FD001_test = subset_test_dfs[0]\n",
    "FD002_test = subset_test_dfs[1]\n",
    "FD003_test = subset_test_dfs[2]\n",
    "FD004_test = subset_test_dfs[3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b6e54dbbf46939",
   "metadata": {},
   "source": "## 3.0 Simulate engine train Cat 001 - Test on cat 001"
  },
  {
   "cell_type": "code",
   "id": "267d7471a1e8acf7",
   "metadata": {},
   "source": [
    "best_model, val_rmse = fit_rf(FD001_train)\n",
    "y_pred, y_test, metrics = eval_rul(best_model, FD001_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24ce85f83171fd1c",
   "metadata": {},
   "source": "## 3.1 Simulate engine train Cat 001 - Test on cat 001 + 002"
  },
  {
   "cell_type": "code",
   "id": "5dd88c5308cdc69f",
   "metadata": {},
   "source": [
    "FD001_2_test = pd.concat([FD001_test, FD002_test], ignore_index=True)\n",
    "\n",
    "y_pred, y_test, metrics = eval_rul(best_model, FD002_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17ac062db1d257fe",
   "metadata": {},
   "source": [
    "FD001_2_train = pd.concat([FD001_train, FD002_train], ignore_index=True)\n",
    "best_model, val_rmse = fit_rf(FD001_2_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64cc65f302e44186",
   "metadata": {},
   "source": [
    "y_pred, y_test, metrics = eval_rul(best_model, FD002_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d63bffbd9c5a2fb4",
   "metadata": {},
   "source": "## 3.2 Simulate engine train Cat 001 + 002 - Test on cat 003"
  },
  {
   "cell_type": "code",
   "id": "fd45b42458f00470",
   "metadata": {},
   "source": [
    "y_pred, y_test, metrics = eval_rul(best_model, FD003_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d3a545288ac5c68",
   "metadata": {},
   "source": [
    "FD001_2_3_train = pd.concat([FD001_train, FD002_train, FD003_train], ignore_index=True)\n",
    "\n",
    "best_model, val_rmse = fit_rf(FD001_2_3_train)\n",
    "\n",
    "y_pred, y_test, metrics = eval_rul(best_model, FD003_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ded1b4b68ddf0e3b",
   "metadata": {},
   "source": "## 3.3 Simulate engine train Cat 001 + 002 + 003 - Test on cat 004"
  },
  {
   "cell_type": "code",
   "id": "6f6f108b376d0f95",
   "metadata": {},
   "source": [
    "y_pred, y_test, metrics = eval_rul(best_model, FD004_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e0e9343b344a6",
   "metadata": {},
   "source": [
    "FD001_2_3_4_train = pd.concat([FD001_train, FD002_train, FD003_train, FD004_train], ignore_index=True)\n",
    "\n",
    "best_model, val_rmse = fit_rf(FD001_2_3_4_train)\n",
    "\n",
    "y_pred, y_test, metrics = eval_rul(best_model, FD004_test)\n",
    "fig = plot_rmse(y_test, y_pred, metrics.rmse)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f856f2798566d7a0",
   "metadata": {},
   "source": "_____"
  },
  {
   "cell_type": "markdown",
   "id": "68152ce030689749",
   "metadata": {},
   "source": "# Strategy : All unit shuffled - train/test same order - eval sliding window - detect drift -> retrain on where we are on test"
  },
  {
   "cell_type": "code",
   "id": "e9bb94fe902e25d7",
   "metadata": {},
   "source": [
    "print(train_to_use[\"unit_number\"].nunique())\n",
    "print(test_to_use[\"unit_number\"].nunique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a162a760065f342",
   "metadata": {},
   "source": [
    "train_units = set(train_to_use[\"unit_number\"].values)\n",
    "test_units = set(test_to_use[\"unit_number\"].values)\n",
    "\n",
    "common_units = train_units.intersection(test_units)  # 1001, 1002, 1003, .... 4001, 4002, etc\n",
    "num_common = len(common_units)\n",
    "\n",
    "print(f\"Number of common unit_numbers: {num_common}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f48b841e5d5f2c5",
   "metadata": {},
   "source": [
    "# Shuffle the common unit numbers\n",
    "common_units_list = list(common_units)\n",
    "# np.random.shuffle(common_units_list)\n",
    "\n",
    "# Take the first 10%\n",
    "num_units_to_take = int(len(common_units_list) * 0.1)\n",
    "selected_units = common_units_list[:num_units_to_take]\n",
    "\n",
    "train_subset = train_to_use[train_to_use[\"unit_number\"].isin(selected_units)]\n",
    "\n",
    "\n",
    "test_subset = test_to_use[test_to_use[\"unit_number\"].isin(selected_units)]\n",
    "\n",
    "best_model, val_rmse = fit_rf(train_subset)\n",
    "y_pred, y_test, metrics = eval_rul(best_model, test_subset)\n",
    "# fig = plot_rmse(y_test, y_pred, rmse)\n",
    "# fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f55efa2ee062e59",
   "metadata": {},
   "source": [
    "# Shuffle the common unit numbers\n",
    "common_units_list = list(common_units)\n",
    "# np.random.shuffle(common_units_list)\n",
    "\n",
    "# Parameters\n",
    "train_ratio = 0.1  # Fixed training ratio\n",
    "eval_portion = 0.25  # Random portion of available units to actually evaluate on\n",
    "step_size = 0.025  # Increase evaluation pool by 5% each iteration\n",
    "max_ratio = 1.0  # Go up to 100%\n",
    "\n",
    "# Storage for results\n",
    "eval_pool_ratios = []\n",
    "rmse_values = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "529f6a3fe4ae0de2",
   "metadata": {},
   "source": [
    "# Train model once on fixed ratio\n",
    "print(f\"Training model on {train_ratio * 100:.1f}% of units...\")\n",
    "num_units_to_take = int(len(common_units_list) * train_ratio)\n",
    "selected_units = common_units_list[:num_units_to_take]\n",
    "\n",
    "train_subset = train_to_use[train_to_use[\"unit_number\"].isin(selected_units)]\n",
    "\n",
    "# Train the model\n",
    "best_model, val_rmse = fit_rf(train_subset)\n",
    "if val_rmse is not None:\n",
    "    print(f\"Training complete on {len(selected_units)} units. Validation RMSE: {val_rmse:.4f}\")\n",
    "else:\n",
    "    print(f\"Training complete on {len(selected_units)} units.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21b35e200757ad15",
   "metadata": {},
   "source": [
    "# Calculate evaluation pool ratios\n",
    "current_ratio = train_ratio + step_size  # Start after training ratio\n",
    "eval_ratios = []\n",
    "eval_ratios.append(train_ratio)\n",
    "while current_ratio <= max_ratio:\n",
    "    eval_ratios.append(current_ratio)\n",
    "    current_ratio += step_size\n",
    "\n",
    "print(f\"Will evaluate with pool ratios: {[r * 100 for r in eval_ratios]}%\")\n",
    "\n",
    "test_subset = test_to_use[test_to_use[\"unit_number\"].isin(selected_units)]\n",
    "\n",
    "# Track retraining points\n",
    "retrain_ratios = []\n",
    "\n",
    "for eval_ratio in eval_ratios:\n",
    "    num_units_to_take = int(len(common_units_list) * eval_ratio)\n",
    "    selected_units = common_units_list[:num_units_to_take]\n",
    "\n",
    "    test_subset = test_to_use[test_to_use[\"unit_number\"].isin(selected_units)]\n",
    "\n",
    "    test_subset_sample = test_subset.sample(frac=eval_portion, random_state=42)\n",
    "\n",
    "    if len(test_subset) > 0:\n",
    "        # Evaluate the model\n",
    "        y_pred, y_test, metrics = eval_rul(best_model, test_subset_sample)\n",
    "\n",
    "        eval_pool_ratios.append(eval_ratio)\n",
    "        rmse_values.append(metrics.rmse)\n",
    "\n",
    "        if metrics.rmse > 29.0:  # dummy drift detection\n",
    "            train_subset = train_to_use[train_to_use[\"unit_number\"].isin(selected_units)]\n",
    "            best_model, _ = fit_rf(train_subset)\n",
    "            # Track this retraining point\n",
    "            retrain_ratios.append(eval_ratio)\n",
    "    else:\n",
    "        assert (False, f\"No test samples found for evaluation pool at {eval_ratio * 100:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20f2fc43c04d27d4",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([r * 100 for r in eval_pool_ratios], rmse_values, \"b-o\", linewidth=2, markersize=6)\n",
    "plt.xlabel(\"Evaluation Pool Size (% of Total Units)\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\n",
    "    f\"Model Performance vs Evaluation Pool Size\\n\"\n",
    "    f\"(Trained on {train_ratio * 100:.1f}% units, Each evaluation uses {eval_portion * 100:.1f}% of available pool)\"\n",
    ")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add vertical line showing training size\n",
    "plt.axvline(\n",
    "    x=train_ratio * 100, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Training size ({train_ratio * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Add red lines at each actual retraining point\n",
    "for i, retrain_ratio in enumerate(retrain_ratios):\n",
    "    if i == 0:  # Add label only to first retraining point\n",
    "        plt.axvline(x=retrain_ratio * 100, color=\"red\", linestyle=\":\", alpha=0.7, label=\"Model retrained\")\n",
    "    else:\n",
    "        plt.axvline(x=retrain_ratio * 100, color=\"red\", linestyle=\":\", alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if rmse_values:\n",
    "    plt.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        f\"Min RMSE: {min(rmse_values):.4f}\\nMax RMSE: {max(rmse_values):.4f}\\nFinal RMSE: {rmse_values[-1]:.4f}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "plt.savefig(os.path.join(config.TEMP_FOLDER, \"drift_simulation_notebook.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Model trained on: {train_ratio * 100:.1f}% of units ({len(train_units)} units)\")\n",
    "print(f\"Evaluation portion: {eval_portion * 100:.1f}% of each pool (random selection)\")\n",
    "print(f\"Step size: {step_size * 100:.1f}%\")\n",
    "if rmse_values:\n",
    "    print(f\"RMSE range: {min(rmse_values):.4f} - {max(rmse_values):.4f}\")\n",
    "    print(f\"Number of evaluation points: {len(rmse_values)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3e978338c113c1b",
   "metadata": {},
   "source": "_____"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
